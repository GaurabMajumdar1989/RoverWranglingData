{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a08c532-b181-44a3-ac3a-8c1cd38d65e2",
   "metadata": {},
   "source": [
    "# Phase 1 â€” Basic LLM Playground\n",
    "\n",
    "This notebook introduces the fundamentals of LLM behavior:\n",
    "- Prompt structure  \n",
    "- Temperature control (deterministic vs creative)  \n",
    "- Role prompting  \n",
    "- Few-shot prompting  \n",
    "- Reasoning scaffolds  \n",
    "- Unified LLM runner  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16d9fad2-ff76-4cbb-9efa-4aac8aeae0da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment Loaded\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "print(\"Environment Loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c41f32b-fc03-4279-959e-04b49605e67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "MODEL_BACKEND = \"gemini\"  \n",
    "# options: \"openai\", \"ollama\", \"transformers\", \"gemini\"\n",
    "\n",
    "\n",
    "def run_llm(\n",
    "    prompt: str,\n",
    "    temperature: float = 0.0,\n",
    "    max_tokens: int = 200,\n",
    "    model: str = \"gpt-4o-mini\"\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Unified LLM Runner for:\n",
    "    - OpenAI\n",
    "    - Gemini (via OpenAI-style client)\n",
    "    - Ollama (local inference)\n",
    "    - Transformers (offline HF models)\n",
    "    \"\"\"\n",
    "    print(f\"MYYYYYYYYYYYY MODDDDDDD ISSSSSSSS {model}\")\n",
    "    # ---------------------------\n",
    "    # BACKEND 1: OpenAI\n",
    "    # ---------------------------\n",
    "    if MODEL_BACKEND == \"openai\":\n",
    "        from openai import OpenAI\n",
    "        client = OpenAI()   # uses OPENAI_API_KEY from env\n",
    "        resp = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=temperature,\n",
    "            max_tokens=max_tokens\n",
    "        )\n",
    "        return resp.choices[0].message[\"content\"]\n",
    "\n",
    "    # ---------------------------\n",
    "    # BACKEND 2: GEMINI (Google)\n",
    "    # ---------------------------\n",
    "    if MODEL_BACKEND == \"gemini\":\n",
    "        import os\n",
    "        from openai import OpenAI\n",
    "\n",
    "        GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "\n",
    "        client = OpenAI(\n",
    "            api_key=GEMINI_API_KEY,\n",
    "            base_url=\"https://generativelanguage.googleapis.com/v1beta/\"\n",
    "        )\n",
    "\n",
    "        resp = client.chat.completions.create(\n",
    "            model=model,  # gemini-2.5-flash\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=temperature,\n",
    "            max_tokens=max_tokens\n",
    "        )\n",
    "        return resp.choices[0].message.content\n",
    "\n",
    "    # ---------------------------\n",
    "    # BACKEND 3: OLLAMA (Local LLM)\n",
    "    # ---------------------------\n",
    "    if MODEL_BACKEND == \"ollama\":\n",
    "        import ollama\n",
    "        resp = ollama.chat(\n",
    "            model=model,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "        return resp[\"message\"][\"content\"]\n",
    "\n",
    "    # ---------------------------\n",
    "    # BACKEND 4: Transformers (HF Offline)\n",
    "    # ---------------------------\n",
    "    if MODEL_BACKEND == \"transformers\":\n",
    "        from transformers import pipeline\n",
    "        generator = pipeline(\"text-generation\", model=model)\n",
    "        out = generator(\n",
    "            prompt,\n",
    "            temperature=temperature,\n",
    "            max_new_tokens=max_tokens\n",
    "        )\n",
    "        return out[0][\"generated_text\"]\n",
    "\n",
    "    raise ValueError(f\"Unknown backend: {MODEL_BACKEND}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41a0813e-bacf-4415-b00b-75b3ce75edc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPENAI: \n",
      "GEMINI: AIzaSyAv9YmxnU4sU0fWw-cu0CnPtR8dcy6z_FE\n",
      "Backend: gemini\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "print(\"OPENAI:\", os.getenv(\"OPENAI_API_KEY\"))\n",
    "print(\"GEMINI:\", os.getenv(\"GEMINI_API_KEY\"))\n",
    "print(\"Backend:\", os.getenv(\"MODEL_BACKEND\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d505e20-2815-4364-9f02-f75556b8beb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MYYYYYYYYYYYY MODDDDDDD ISSSSSSSS gemini-2.5-flash\n",
      "In LLMs, \"\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    run_llm(\n",
    "        \"Explain temperature in LLMs in one short paragraph.\",\n",
    "        temperature=0.2,\n",
    "        max_tokens=120,\n",
    "        model=\"gemini-2.5-flash\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef04c987-1710-4353-b945-c9a9799e55f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e1360d-495d-4796-921f-571a2fa08969",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RWD Kernel",
   "language": "python",
   "name": "rwd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
